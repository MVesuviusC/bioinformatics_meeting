---
title: "Demo of Keras and Tensorflow in R"
author: "Matthew V. Cannon"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
format:
    html:
        toc: true
        toc_float: true
        toc_depth: 5
        number_sections: false
        code-fold: true
        code-link: true
        df-print: kable
        embed-resources: true
        self_contained: true
execute:
    cache: true
knitr:
    opts_chunk:
        cache.lazy: false
        warning: false
        message: false
lightbox: true
---

```{r libraries}
library(tensorflow)
library(keras)
library(Seurat)
library(reticulate)
library(tidyverse)
```

For documentation check out:

https://tensorflow.rstudio.com/

## Goal of the model

We're going to build a model to predict cell types in a scRNAseq dataset. We're going to use a portion of the expressed genes to make this prediction to save on computational time. Once we build a model, we'll evaluate its accuracy and then use it to predict cell types in a test dataset.

## Get training data
From https://data.humancellatlas.org/hca-bio-networks/eye/atlases/retina-v1-0

- scRNA-seq of human retina - all cells
- 5 tissues
- Healthy tissue
- 265.8k cells

See data here: https://cellxgene.cziscience.com/e/be41a86a-b606-4b1c-8055-32f334898775.cxg/

I set this to eval: false so that the data isn't downloaded every time the document is run. You can download the data by running the following command:
```{bash get_raw}
#| eval: false
wget \
    -O neural_networks_2024/input/human_retina.rds \
    https://datasets.cellxgene.cziscience.com/64175889-d600-4b58-97ea-e74be80206e5.rds
```

## Read in the data

```{r read_data}
sobj <- readRDS("input/human_retina.rds")

sobj
```

## Filter the data
We'll downsample the data to only include cells with at least 1000 UMI counts to filter out the worst of the low quality cells. This doesn't preferentially remove any cell types as the low quality cells are distributed across all cell types. I also print out a table of the cell types in the dataset.
```{r downsample}
min_umis_per_cell <- 1000
sobj$crap <- sobj$nCount_RNA < min_umis_per_cell

table(sobj$cell_type, sobj$crap)

sobj <- subset(sobj, crap == FALSE)

sobj$cell_type %>% table()
```

## Get the most variable genes
We don't want to use the entire dataset for this as it would take too long to run. Also, many of the genes in the dataset likely aren't informative for cell type prediction. We'll use only the top 500 most variable genes in the dataset as determined by Seurat. These genes should be the most informative for our model. I'll also re-process the data for downstream analysis.
```{r get_variable_genes}
# Lets use just a portion of the most variable genes
use_n_genes <- 500

sobj <-
    FindVariableFeatures(sobj, nfeatures = use_n_genes) %>%
    ScaleData() %>%
    RunPCA() %>%
    FindNeighbors() %>%
    FindClusters() %>%
    RunUMAP(dims = 1:30)

qs::qsave(sobj, "output/rdata/processed_sobj.qs")
```

## Lets randomly assign the cells to training, testing, and validation sets
We're going to need three datasets for this analysis. We'll use 60% of the data for training, 20% for testing, and 20% for validation. We'll also plot the UMAP of the data colored by cell type and dataset to make sure the cell types are distributed evenly across the datasets.
```{r assign_groups}
set.seed(1337)
sobj$dataset <-
    sample(
        c("train", "test", "validation"),
        prob = c(0.6, 0.2, 0.2),
        nrow(sobj),
        replace = TRUE
    )

sobj$dataset %>% table()

DimPlot(
    sobj,
    group.by = "cell_type",
    split.by = "dataset",
    label = TRUE,
    repel = TRUE,
    raster.dpi = c(1024, 1024)
) +
    NoLegend()

ggsave(
    "output/figures/umap_cell_type.png",
    width = 30,
    height = 10
)
```

## Lets pull out the data and scale it between 0 and 1 to make it friendly for the model
Neural networks don't work well with large numbers and it is best to scale the data between 0 and 1. We'll use the rank of the expression to scale it between 0 and 1. This will make the data more friendly for the model to work with. There are other ways I could have done this, but this is a simple way.

Note how the scaled data is originally well outside of 0 <-> 1, but is scaled to be between 0 and 1 after the scaling operation. I'm grabbbing the scale.data layer from the Seurat object so that lower expressed genes are still informative for the model.
```{r scale_data}
GetAssayData(sobj, layer = "scale.data")[, 1:10] %>%
    summary()

scaled_counts <-
    GetAssayData(sobj, layer = "scale.data") %>%
    apply(
        MARGIN = 2,
        function(x) {
            rank(x) / length(x)
        }
    )

scaled_counts[, 1:10] %>%
    summary()

dim(scaled_counts)
```

## Create a category key
We'll create a label key to keep track of the cell types and their corresponding numbers. Neural networks can only handle numbers, so if your data is text, like cell type labels, you'll need to convert them to integers to use in the model. I'll need to decode these values later on, so I'm creating a handy key variable to keep track of the cell types and their corresponding numbers. I'm also using data_labels later on to keep track of how many cell types I have.
```{r create_category_key}
data_labels <-
    factor(
        as.character(sobj$cell_type),
        levels = sort(as.character(unique(sobj$cell_type)))
    )

data_numbers <- as.numeric(data_labels) - 1

label_key <-
    tibble(
        label_name = data_labels,
        label_number = data_numbers
    ) %>%
    distinct() %>%
    pull(label_name, name = "label_number")
```

## Split the data into training, testing, and validation sets
We're going to need three datasets in this demo. We'll use 60% of the data for training, 20% for testing, and 20% for validation.

The training data will be used to train the model, naturally.

The validation data will be used to tune the model's hyperparameters during model fitting.

We use the testing data to evaluate how well the model generalizes to new data and check if the model is overfit.
```{r split_data}
train_data <- scaled_counts[, sobj$dataset == "train"]
train_label_numbers <- data_numbers[sobj$dataset == "train"]

test_data <- scaled_counts[, sobj$dataset == "test"]
test_label_numbers <- data_numbers[sobj$dataset == "test"]

validation_data <- scaled_counts[, sobj$dataset == "validation"]
validation_label_numbers <- data_numbers[sobj$dataset == "validation"]
```

## Lets build a simple neural network to predict cell type from the expression data
I'm going to use a couple variables here to make sure the model can use my input data and I don't have to manually change these values.

`number_of_cell_types` is the number of unique cell types in the dataset. This is used to determine the number of output nodes in the model.

`use_n_genes` is the number of genes we're using in the model. This is the number of genes we used to scale the data earlier on. This is used to determine the number of input nodes in the model.

Building a model with keras is very similar to building a plot with ggplot2. You start with a base model and then add layers to it. Here I'm using a simple model with two hidden layers and a softmax output layer. We define the input_shape in the first dense layer to match the number of genes in our scaled data. When we train the model, the data from each cell will be fed into the model, which is why the input is equal to the number of genes. The final layer will output the probability of each cell type for each cell, which is why the number of output nodes is equal to the number of cell types.

The model is compiled with the adam optimizer and sparse categorical crossentropy loss function. The adam optimizer is a popular optimizer for neural networks that handles gradient descent for you.

The sparse categorical crossentropy loss function is used for classification problems where the output is a single integer representing the class. This is a good loss function for this dataset as the cell types are represented as integers. You may need to use a different loss function if your data is represented differently.

We begin by defining the structure of the model. This doesn't build or train the data (we'll do that later). This is just getting things set up.

Compiling the model sets the optimizer, loss function, and metrics for the model. This is where we define how the model will be trained. The optimizer is the algorithm that will be used to minimize the loss function. The loss function is the function that the model will try to minimize. The metrics are the values that will be printed out as the model trains. Here we're using accuracy as the metric to print out as the model trains.
```{r build_model}
number_of_cell_types <- length(unique(data_labels))
model <-
    keras_model_sequential() %>%
    layer_dense(
        units = 128,
        activation = "relu",
        input_shape = use_n_genes
    ) %>%
    layer_dense(
        units = 64,
        activation = "relu"
    ) %>%
    layer_dense(
        units = number_of_cell_types,
        activation = "softmax"
    )

model %>%
    compile(
        optimizer = 'adam',
        loss = 'sparse_categorical_crossentropy',
        metrics = c('accuracy')
    )

model
```

## Fit the model
This is where we actually train the model. We use the fit function to train the model on the training data. We provide a matrix of the training data where each cell is a  We use the verbose argument to print out the progress of the model as it trains. This is useful to see how the model is doing and if it's converging. We're only doing 20 epochs here to save time, but you may need to train the model for many more to get a good model. We'll evaluate the model's accuracy later on to see how well it's doing.
```{r fit_model}
fit_history_simple <-
    model %>%
    fit(
        t(train_data),
        train_label_numbers,
        epochs = 20,
        verbose = 2
    )
```

## Evaluate accuracy
Let's see how well our model performs. We got some idea from the verbose output of the model as it trained, but we can get a more detailed look at the model's performance by evaluating it on the *test data*. We'll use the evaluate function to evaluate the model on the test data. This will give us the loss and accuracy of the model on the test data. We can use this information to see how well the model generalizes to new data. If we are overfitting, the model will perform poorly on the test data compared to the training data. If the model is underfitting, the model will perform poorly on both the training and test data. We can also plot the model's accuracy and loss over time to see how the model is converging to assess if we should have trained the model longer.
```{r evaluate}
fit_history_simple %>%
    plot() +
    coord_cartesian(xlim = c(0, 30))

ggsave(
    "output/figures/fit_history_simple.png",
    width = 10,
    height = 10
)

model %>%
    evaluate(
        t(test_data),
        test_label_numbers,
        verbose = 0
    )
```

## Predict cell types
Our model works ok, so lets try using it to predict cell types in the test dataset. We'll use the predict function to predict the cell types in the test data. This will give us a matrix of probabilities for each cell type for each cell. We'll then pick the cell type with the highest probability for each cell and use that as the predicted cell type. We'll then add the predicted cell types to the Seurat object and plot the predicted cell types on the UMAP. We'll also print out a confusion matrix to see how well the model predicted the cell types.
```{r predict}
predictions <-
    model %>%
    predict(t(test_data))

predicted_labels <-
    parallel::mclapply(
        seq_len(nrow(predictions)),
        mc.cores = parallel::detectCores(),
        function(i) {
            x <- predictions[i,]
            tibble(
                max_call = which.max(x),
                picked_label = label_key[as.character(max_call - 1)]
            )
        }
    ) %>%
    bind_rows()

predicted_labels <-
    data.frame(
        predicted_cell_type = predicted_labels$picked_label,
        row.names = colnames(test_data)
    )

test_sobj <-
    subset(sobj, dataset == "test") %>%
    AddMetaData(
        metadata = predicted_labels,
        col.name = "predicted_cell_type"
    )

test_sobj$predicted_cell_type <-
    factor(
        test_sobj$predicted_cell_type,
        levels = levels(test_sobj$cell_type)
    )

(DimPlot(
    test_sobj,
    group.by = "cell_type",
    label = TRUE,
    repel = TRUE,
    raster.dpi = c(1024, 1024)
) +
    NoLegend()) +
(DimPlot(
    test_sobj,
    group.by = "predicted_cell_type",
    label = TRUE,
    repel = TRUE,
    raster.dpi = c(1024, 1024)
) +
    NoLegend())

ggsave(
    "output/figures/umap_predicted_cell_type.png",
    width = 30,
    height = 10
)

table(test_sobj$cell_type, test_sobj$predicted_cell_type) %>%
    as.data.frame() %>%
    rename(
        actual = Var1,
        predicted = Var2,
        count = Freq
    ) %>%
    mutate(actual = as.character(actual),
           predicted = as.character(predicted)) %>%
    ggplot(aes(x = actual, y = predicted, fill = count)) +
    geom_tile() +
    geom_text(aes(label = count), vjust = 1) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

ggsave(
    "output/figures/confusion_matrix.png",
    width = 10,
    height = 10
)
```

## Add a dropout layer
We have a lot of genes that are being fed into our model. This can lead to overfitting as the model may learn the noise in the data instead of the signal. To help prevent overfitting, we can add dropout layers to the model. Dropout layers randomly set a fraction of the input units to 0 at each step during training. This helps prevent the model from learning the noise in the data and helps the model generalize better to new data. We'll add a dropout layer that sets 50% of the input units to 0.
```{r add_dropout}
dropout_model <-
    keras_model_sequential() %>%
    layer_dense(
        units = 128,
        activation = "relu",
        input_shape = use_n_genes
    ) %>%
    layer_dropout(0.5) %>%
    layer_dense(
        units = 64,
        activation = "relu"
    ) %>%
    layer_dense(
        units = length(unique(data_labels)),
        activation = "softmax"
    )

dropout_model %>%
    compile(
        optimizer = 'adam',
        loss = 'sparse_categorical_crossentropy',
        metrics = c('accuracy')
    )
```

## Fit the model with dropout layers using validation data and early stopping
We're going to do a few things differently here.

This time we will include validation data in the model fitting. During training, the model will train only on the training data still, but will evaluate the model on the validation data at each epoch. This will help us tune the model's hyperparameters as we try re-training our model so that our test dataset is a true test of the model's performance.

We'll also add an early stopping callback to stop the model from training if the validation accuracy doesn't improve for 3 epochs. This will help prevent the model from overfitting to the training data. We'll also restore the best weights of the model when training stops. This will give us the best model possible.
```{r fit_dropout}
model_fit_data <-
    fit(
        dropout_model,
        t(train_data),
        train_label_numbers,
        validation_data = list(t(validation_data), validation_label_numbers),
        epochs = 200,
        callbacks = callback_early_stopping(
            monitor = 'val_accuracy',
            patience = 5,
            min_delta = 0.01,
            restore_best_weights = TRUE
        ),
        verbose = 2
    )

model_fit_data %>%
    plot() +
    coord_cartesian(xlim = c(0, 30))

ggsave(
    "output/figures/fit_history_dropout.png",
    width = 10,
    height = 10
)

evaluate(dropout_model, t(train_data), train_label_numbers)
evaluate(dropout_model, t(test_data), test_label_numbers)
```

## Lets add a learning rate schedule so that the model takes smaller steps as it gets closer to optimum
By default, the learning rate of the model is the same as it trains. This can be problematic as the model gets more refined and closer to the optimum. Each step the model takes may end up overshooting optimal values over and over. By using a learning rate schedule, we can make the model take smaller steps as it gets closer to the optimum. This can help the model converge faster and more accurately.

To do this, we first define a learning schedule. Here we use the inverse time decay schedule. This schedule takes the initial learning rate and divides it by the decay rate raised to the power of the number of steps divided by the decay steps. This will make the learning rate decrease as the number of steps increases. We provide this learning schedule when compiling the model to the "optimizer" argument.

All of the hyperparameters here can be adjusted to fit the model and data.

https://tensorflow.rstudio.com/guides/keras/training_with_built_in_methods#using-learning-rate-schedules
```{r slow_learning}
steps_per_epoch <- 100

lr_schedule <-
    learning_rate_schedule_inverse_time_decay(
        initial_learning_rate = 0.001,
        decay_steps = 100000,
        decay_rate = 5,
        staircase = FALSE
    )

dropout_model %>%
    compile(
        optimizer = optimizer_adam(lr_schedule),
        loss = 'sparse_categorical_crossentropy',
        metrics = c('accuracy')
    )

fit_history <-
    dropout_model %>%
    fit(
        t(train_data),
        train_label_numbers,
        validation_data = list(t(validation_data), validation_label_numbers),
        epochs = 10,
        verbose = 2
    )

fit_history %>%
    plot() +
    coord_cartesian(xlim = c(0, 30))

ggsave(
    "output/figures/fit_history_slow_learning.png",
    width = 10,
    height = 10
)

evaluate(dropout_model, t(test_data), test_label_numbers)
```

Note how the plot of the model accuracy and loss shows that the model converges faster and more accurately with the learning rate schedule. This is a good example of how hyperparameter tuning can improve model performance.

Some further considerations:

- We are assuming that the data are properly annotated in the first place. Hopefully this is true. But if the annotations are wrong, that may explain why our confusion matrix is not perfect. Our model could also be wrong, but it's worth checking the annotations.
- We used the top 500 genes. This may have been not enough genes to predict the cell types accurately. We could try using more genes to see if that improves the model without impacting computational time or overfitting.
- I didn't really try many model architectures. You'd likely want to try many different architectures to see which one works best for your data. This is a good starting point, but you'd likely want to try many more models to see which one works best. The validation data can help with this so that you can test the model's performance on new data once you think it's working well.
- Batching your training data can help speed up training. I didn't do this here, but you may want to try this to see if it speeds up training.

